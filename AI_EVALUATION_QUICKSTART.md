# ğŸ¤– AI Auto-Evaluation Quick Start

## What's New?

The Admin Feedback System now includes **automatic AI evaluation** using GPT-4o-mini to assess bot responses and prioritize conversations needing human review.

## Key Features

âœ… **Automatic Quality Scoring** - Each response gets a 0-10 score  
âœ… **Priority Classification** - Critical/High/Medium/Low badges  
âœ… **Smart Filtering** - Show only conversations needing review  
âœ… **Detailed Analysis** - Issues, strengths, and category scores  
âœ… **Efficient Workflow** - Focus on conversations that matter  

## Quick Setup (1 minute)

### 1. Verify Configuration

The system uses your existing Azure OpenAI configuration:
- `AZURE_OPENAI_ENDPOINT` - Already configured âœ…
- `AZURE_OPENAI_KEY` - Already configured âœ…
- Deployment: `gpt-4o-mini` - Uses your existing deployment

> No additional configuration needed! The AI evaluator uses the same Azure OpenAI resource.

### 2. Restart Server (if needed)

```bash
cd server
python server.py
```

Look for: `INFO - AI evaluator initialized`

### 3. Use in Admin Dashboard

1. Open: `http://localhost:5000/static/admin/index.html`
2. Click **"ğŸ¤– Auto-Evaluate All"** button
3. See priority badges on conversations:
   - ğŸš¨ **Critical** - Needs immediate review
   - âš ï¸ **High** - Should review soon
   - â„¹ï¸ **Medium** - Optional review
   - âœ… **Good** - Quality is fine

4. Enable **"ğŸ¤– Only AI-flagged for review"** checkbox to filter

## Usage Workflow

```
1. Auto-Evaluate â†’ 2. Filter Critical â†’ 3. Review AI Analysis â†’ 4. Add Human Feedback
```

### Example UI

**Conversation List:**
```
Conv-123  [Web]  ğŸš¨ Critical (3/10)  â†’ Click to review
Conv-456  [Phone]  âœ… Good (9/10)   â†’ Skip review
Conv-789  [Web]  âš ï¸ High (5/10)     â†’ Should review
```

**Conversation Details:**
```
Turn 1: âœ… Good (9/10)
  âœ“ Accurate information
  âœ“ Professional tone
  
Turn 2: ğŸš¨ Critical (3/10)
  âœ— Factual error detected
  âœ— Incomplete answer
  â†’ Add human feedback here!
```

## API Endpoints

```bash
# Evaluate entire conversation
POST /admin/api/evaluate/{conversation_id}

# Evaluate single turn
POST /admin/api/evaluate/{conversation_id}/{turn_number}

# Get stored evaluation
GET /admin/api/evaluations/{conversation_id}
```

## Evaluation Criteria

Each response is scored on:

| Category | What's Checked |
|----------|----------------|
| **Accuracy** | Factual correctness |
| **Tone** | Professional & empathetic |
| **Context** | Conversation awareness |
| **Completeness** | Fully answers question |
| **Clarity** | Clear & understandable |

## Cost

- ~$0.0002-0.0005 per conversation
- 1000 evaluations â‰ˆ $0.20-0.50
- Cached for 24 hours (no duplicate costs)

## Full Documentation

ğŸ“– See [AI_EVALUATION_SYSTEM.md](./AI_EVALUATION_SYSTEM.md) for complete details  
âš™ï¸ See [AI_EVALUATION_CONFIG.md](./AI_EVALUATION_CONFIG.md) for setup guide

## Troubleshooting

**"AI evaluator not initialized"**  
â†’ Check `AZURE_OPENAI_ENDPOINT` and `AZURE_OPENAI_KEY` are set  
â†’ Verify Azure OpenAI resource is accessible

**No priority badges showing**  
â†’ Click "ğŸ¤– Auto-Evaluate All" first

**Evaluation failed**  
â†’ Verify `gpt-4o-mini` deployment exists in your Azure OpenAI resource  
â†’ Check Azure OpenAI quota and rate limits

---

**Happy Reviewing! ğŸ¯**
